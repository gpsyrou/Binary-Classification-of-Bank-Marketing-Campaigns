{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title: Binary classification of subscriptions for direct marketing campaigns: A logistic regression & random forest approach\n",
    "### Author: Georgios Spyrou (georgios.spyrou1@gmail.com)\n",
    "### Completion Date: 04/08/2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Purpose of this project is to analyze a dataset containing information about marketing campaigns that were conducted via phone calls from a Portuguese banking institution to their clients. Main goal of these campaigns was to attempt and persuade their clients to subscribe to a specific product (bank term deposit). At the end of each call the client was either ended up subscibing to the product (successful campaing) or not (unsuccessful campaing). \n",
    "\n",
    "Our main goal in this project is to create machine learning algorithms that are able to predict the probability of a client subscribing to the bank's product.\n",
    "\n",
    "The dataset contains 41188 instances of calls to clients (rows) and 21 features (columns) describing certain aspects of the call. Please note that there are cases where the same client was contacted multiple times - something that practically doesn't affect our analysis as each call is being considered indipendent from another. If a client has been contacted multiple times is being captured in the features, as presented below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables\n",
    "\n",
    "The predictor variables (features) contained in the dataset and that we will use in order to predict the probability that a client will subscribe for the product, can be divided into 5 sections:\n",
    "\n",
    "1. Features describing the bank client:\n",
    "\n",
    "    a. **age** <br>\n",
    "    b. **job**: type of job (_e.g. 'admin', 'technician', 'unemployed', etc_) <br>\n",
    "    c. **marital**: marital status (_'married', 'single', 'divorced', 'unknown'_) <br>\n",
    "    d. **education**: education level (_'basic.4y', 'high.school', 'basic.6y', 'basic.9y','professional.course', 'unknown','university.degree','illiterate'_) <br>\n",
    "    e. **default**: has credit in default ? (_'no', 'unknown', 'yes'_) <br>\n",
    "    f. **housing**: has housing loan ? (_'no', 'unknown', 'yes'_)<br>\n",
    "    g. **loan**: has personal loan ? (_'no', 'unknown', 'yes'_)<br>\n",
    "    <br>\n",
    "2. Features related to the last contact of the current campaign:\n",
    "\n",
    "    a. **contact**: contact communication type (_'telephone', 'cellular'_) <br>\n",
    "    b. **month**: last contact month of year <br>\n",
    "    c. **day_of_week**: last contact day of the week <br>\n",
    "    d. **duration**: last contact duration, in seconds <br>\n",
    "    <br>\n",
    "3. Other features related to the campaign(s):\n",
    "\n",
    "    a. **campaign**: # of contacts performed during this campaign and for this client <br>\n",
    "    b. **pdays**: # of days that passed by after the client was last contacted from a previous campaign <br>\n",
    "    c. **previous**: # of contacts performed before this campaign and for this client <br>\n",
    "    d. **poutcome**: outcome of the previous marketing campaign (_'nonexistent', 'failure', 'success'_) <br>\n",
    "    <br>   \n",
    "4. Social and Economic features:\n",
    "\n",
    "    a. **emp.var.rate**: employement variation rate - quarterly indicator <br>\n",
    "    b. **cons.price.idx**: consumer price index - monthly indicator <br>\n",
    "    c. **cons.conf.idx**: consumer confidence index - monthly indicator <br>\n",
    "    d. **euribor3m**: euribor 3 month rate - daily indicator <br>\n",
    "    e. **nr.employed**: # of employees - quarterly indicator <br>\n",
    "      \n",
    "Of course we can also observe the variable **subscribed** which is the target variable for our project, indicating if the client subscribed to the product (_'yes'_) or not (_'no'_).\n",
    "\n",
    "Now that we got a brief understanding of what the project is about, what the variables in our dataset are and what is our main goal we can start with the main part of our analysis. In the first part of this project we will perform all the regular operations needed in pretty much any data science project, i.e. load the dataset into a dataframe, search for corrupt/inaccurate records & perform data cleaning operations and finally perform exploratoty data analysis to identify interesting patterns in the data that might be useful when creating the machine learning algorithms, as well as get to know the features better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Data Import, Cleaning and Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step is to import the relevant libraries that we will use across the whole project, set the project directory where all files are located and load our dataset into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Data Preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "# Classifiers\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "sns.set_style(\"dark\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Project set-up and data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location of the project folder and the dataset (csv file)\n",
    "project_folder = r'C:\\Users\\george\\Desktop\\FCA_Project'\n",
    "\n",
    "os.chdir(project_folder)\n",
    "\n",
    "data_file_loc = os.path.join(project_folder, 'data set.csv')\n",
    "\n",
    "bank_marketing_df = pd.read_csv(data_file_loc, sep=';', header=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'job', 'marital', 'education', 'default', 'housing', 'loan',\n",
       "       'contact', 'month', 'day_of_week', 'duration', 'campaign', 'pdays',\n",
       "       'previous', 'poutcome', 'emp.var.rate', 'cons.price.idx',\n",
       "       'cons.conf.idx', 'euribor3m', 'nr.employed', 'y'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bank_marketing_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that the last column of the dataset ('y') is our target variable which indicates if a client ended up subscribing to the product or not. The initial name from the dataset is not very self-explanatory so we will rename it to something more relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_marketing_df.rename(columns={'y': 'subscribed'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In every data science project (and in general when working with data) before we perform any type of data analysis or create prediction models it's very important that we make sure that our dataset is \"clean\". When calling a dataset clean, we generally mean that we have handled any cases where we had missing values (the might come in the form of NULL if the value is missing or with other types like 'unknown'). Some other data cleaning techniques might include getting rid of duplicate rows as they do not add any value, if working with text check for mispelled words, etc.\n",
    "\n",
    "In general the data cleaning steps are very important for every project, as we want to make sure that our dataset is in it's best shape before we build any model around it. That said, there is no specific steps that we need to take for data cleaning and it's something that the data analyst/scientist should adjust depending on the dataset that they are working with and the specific problem that they are trying to tackle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 41188 rows and 21 columns in the dataset\n"
     ]
    }
   ],
   "source": [
    "# Check if the dataframe has duplicates and remove them\n",
    "df_shape = bank_marketing_df.shape\n",
    "print(f'There are {df_shape[0]} rows and {df_shape[1]} columns in the dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 41176 rows and 21 columns in the dataset after removing duplicates\n"
     ]
    }
   ],
   "source": [
    "bank_marketing_df = bank_marketing_df.drop_duplicates()\n",
    "df_shape = bank_marketing_df.shape\n",
    "print(f'''There are {df_shape[0]} rows and {df_shape[1]} columns in the dataset after removing duplicates''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we can see that we have removed 12 rows from the initial dataset as they were duplicates of other rows and they did not give us any additinal information that we didn't have already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age               0\n",
       "job               0\n",
       "marital           0\n",
       "education         0\n",
       "default           0\n",
       "housing           0\n",
       "loan              0\n",
       "contact           0\n",
       "month             0\n",
       "day_of_week       0\n",
       "duration          0\n",
       "campaign          0\n",
       "pdays             0\n",
       "previous          0\n",
       "poutcome          0\n",
       "emp.var.rate      0\n",
       "cons.price.idx    0\n",
       "cons.conf.idx     0\n",
       "euribor3m         0\n",
       "nr.employed       0\n",
       "subscribed        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identify how many missing values we have per column\n",
    "bank_marketing_df.isnull().sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our check for missing values indicates that all of our rows/columns are fully populated (i.e. no NULL values). That said, this does not mean that we don't have any missing values in another format. For example - as we are going to see below - there are many cases where for specific columns we have values described as 'unknown' indicating that we do not know the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "job           330\n",
       "marital        80\n",
       "education    1730\n",
       "default      8596\n",
       "housing       990\n",
       "loan          990\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There are many missing values in other forms like 'unknown'\n",
    "unknown_cols = bank_marketing_df.isin(['unknown']).sum(axis=0)\n",
    "unknown_cols[unknown_cols > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the time being, we will leave these columns as they are and we won't proceed with imputation methods in order to infer what the values would be. This is something that we might want to do in future iterations of the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
